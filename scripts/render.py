#!/usr/bin/env -S uv run
# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "PyYAML>=6.0",
#     "Jinja2>=3.1",
# ]
# ///
"""
Generic Values Renderer

This script renders configuration values by:
1. Reading config.yaml to get the list of shards and their configurations
2. For each shard and cluster type, loading defaults from existing values files
3. Merging shard-specific overrides with defaults
4. Outputting merged values files to deploy/{environment}/{region_alias}/argocd/{clustertype}-values.yaml
5. Generating terraform pipeline configs to deploy/{environment}/{region_alias}/terraform/
"""

import json
import sys
import shutil
from pathlib import Path
from typing import Any, Dict, List

import yaml
from jinja2 import Environment


def load_yaml(file_path: Path) -> Dict[str, Any]:
    """Load and parse a YAML file.

    Args:
        file_path: Path to the YAML file

    Returns:
        Parsed YAML content as a dictionary
    """
    if not file_path.exists():
        return {}

    with open(file_path, 'r') as f:
        return yaml.safe_load(f) or {}


def validate_shard_uniqueness(shards):
    """Ensure environment + region_alias combinations are unique across shards.

    Args:
        shards: List of shard configurations

    Raises:
        ValueError: If duplicate environment + region_alias combinations are found
    """
    seen_combinations = set()
    for shard in shards:
        combination = (shard.get('environment'), shard.get('region_alias'))
        if combination in seen_combinations:
            raise ValueError(f"Duplicate environment + region_alias combination: {combination}")
        seen_combinations.add(combination)


def validate_config_revisions(shards):
    """Validate that specified config revisions are valid git commit hashes.

    Args:
        shards: List of shard configurations

    Raises:
        ValueError: If a specified config revision is not a valid git commit hash format
    """
    import re

    # Git commit hash pattern (7-40 hex characters)
    commit_hash_pattern = re.compile(r'^[a-f0-9]{7,40}$')

    for shard in shards:
        region_alias = shard.get('region_alias', 'unknown')
        environment = shard.get('environment', 'unknown')
        config_revisions = shard.get('config_revision', {})

        for cluster_type, revision in config_revisions.items():
            if revision:  # Only validate if revision is specified
                if not commit_hash_pattern.match(revision):
                    raise ValueError(
                        f"Invalid commit hash for shard {region_alias} ({environment}): "
                        f"'{revision}' (cluster_type: {cluster_type}). "
                        f"Expected 7-40 character hexadecimal string."
                    )


def save_yaml(data: Dict[str, Any], file_path: Path, cluster_type: str, shard: Dict[str, Any]) -> None:
    """Save a dictionary as a YAML file with proper headers.

    Args:
        data: Dictionary to save
        file_path: Path where to save the YAML file
        cluster_type: Type of cluster (management-cluster, regional-cluster, etc.)
        shard: Shard configuration
    """
    file_path.parent.mkdir(parents=True, exist_ok=True)

    # Generate header
    region_alias = shard['region_alias']
    environment = shard['environment']

    header = f"""# GENERATED FILE - DO NOT EDIT MANUALLY
#
# This file is automatically generated by the render script.
# To make changes:
# - For default changes: Edit values.yaml files in argocd/config/{cluster_type}/*/values.yaml or argocd/config/shared/*/values.yaml
# - For shard-specific changes: Edit config.yaml
# - Then run: scripts/render.py
#
# Cluster Type: {cluster_type}
# Shard: {region_alias} ({environment})
# Generated: {Path(__file__).name}
#

"""

    with open(file_path, 'w') as f:
        f.write(header)
        yaml.dump(data, f, default_flow_style=False, sort_keys=False, allow_unicode=True, width=float('inf'))


def deep_merge(base: Dict[str, Any], overlay: Dict[str, Any]) -> Dict[str, Any]:
    """Recursively merge two dictionaries.

    Args:
        base: Base dictionary
        overlay: Dictionary to merge into base

    Returns:
        Merged dictionary
    """
    result = base.copy()

    for key, value in overlay.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value

    return result


def resolve_templates(value: Any, context: Dict[str, Any]) -> Any:
    """Recursively resolve Jinja2 template placeholders in all string values.

    Args:
        value: Value to process (string, dict, list, or other)
        context: Dictionary of template variables (e.g. aws_region, environment)

    Returns:
        Value with all template placeholders resolved
    """
    if isinstance(value, str):
        return Environment().from_string(value).render(context)
    elif isinstance(value, dict):
        return {k: resolve_templates(v, context) for k, v in value.items()}
    elif isinstance(value, list):
        return [resolve_templates(item, context) for item in value]
    return value


def resolve_shards(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Resolve shards by merging sector defaults and processing templates.

    Each shard's "sector" field links to a sector by name. The sector's
    values, terraform_vars, and environment are inherited by the shard.
    Shard-level settings override sector defaults via deep merge. All
    string values are then template-processed using shard fields as context.

    Args:
        config: Full parsed config.yaml

    Returns:
        List of fully resolved shard configurations
    """
    sectors = config.get('sectors', [])
    sectors_by_name = {s['name']: s for s in sectors}
    shards = config.get('shards', [])

    resolved = []
    for shard in shards:
        shard = dict(shard)  # shallow copy to avoid mutating original
        sector_name = shard.get('sector')
        sector = sectors_by_name.get(sector_name, {})

        # Inherit environment from sector
        shard['environment'] = sector.get('environment', sector_name)

        # Populate region_alias and aws_region from shard fields
        shard['region_alias'] = shard.get('region_alias', shard.get('region', ''))
        shard['aws_region'] = shard.get('aws_region', shard.get('region_alias', ''))

        # Keep 'region' in template context as alias for aws_region (backward compat)
        shard['region'] = shard['aws_region']

        # Deep merge: sector values are defaults, shard values override
        sector_values = sector.get('values', {})
        shard_values = shard.get('values', {})
        shard['values'] = deep_merge(sector_values, shard_values)

        # Deep merge terraform_vars
        sector_tf_vars = sector.get('terraform_vars', {})
        shard_tf_vars = shard.get('terraform_vars', {})
        shard['terraform_vars'] = deep_merge(sector_tf_vars, shard_tf_vars)

        # Template-process values, terraform_vars, and management_clusters using shard fields as context
        shard['values'] = resolve_templates(shard['values'], shard)
        shard['terraform_vars'] = resolve_templates(shard['terraform_vars'], shard)
        shard['management_clusters'] = resolve_templates(shard.get('management_clusters', []), shard)

        resolved.append(shard)

    return resolved


def get_cluster_types(base_dir: Path) -> List[str]:
    """Discover cluster types by looking at directories.

    Args:
        base_dir: Base directory to scan

    Returns:
        List of cluster type names
    """
    cluster_types = []
    for item in base_dir.iterdir():
        if item.is_dir() and not item.name.startswith('.') and item.name.endswith('cluster'):
            cluster_types.append(item.name)
    return cluster_types


def create_applicationset_template(cluster_type: str, environment: str, region_alias: str, config_revision: str = None, base_dir: Path = None) -> Dict[str, Any]:
    """Create ApplicationSet YAML template with specific commit hash or default revision.

    Args:
        cluster_type: Type of cluster (management-cluster, regional-cluster, etc.)
        environment: Environment name
        region_alias: Region alias identifier
        config_revision: Optional commit hash for versioned deployments
        base_dir: Base directory path (for loading base ApplicationSet)

    Returns:
        ApplicationSet dictionary
    """
    # Always load the base ApplicationSet as the starting point
    base_applicationset_path = base_dir / 'applicationset' / 'base-applicationset.yaml'
    if not base_applicationset_path.exists():
        raise ValueError(f"Base ApplicationSet not found: {base_applicationset_path}")

    applicationset = load_yaml(base_applicationset_path)

    # If config_revision is specified, override the git revision to use the specific commit hash
    if config_revision:
        # Find the git generator in the matrix and update its revision
        generators = applicationset['spec']['generators'][0]['matrix']['generators']
        for generator in generators:
            if 'git' in generator:
                # Override revision with specific commit hash
                generator['git']['revision'] = config_revision
                break

        # Update only the first source (chart + values.yaml) to use the specific commit hash
        # The second source (ref: values) should keep using metadata.annotations.git_revision
        sources = applicationset['spec']['template']['spec']['sources']
        for i, source in enumerate(sources):
            if 'targetRevision' in source and 'ref' not in source:
                # This is the chart source (first source) - update to use config_revision
                source['targetRevision'] = config_revision

    return applicationset


def render_shard_applicationsets(
    shard: Dict[str, Any],
    cluster_types: List[str],
    deploy_dir: Path,
    base_dir: Path
) -> None:
    """Generate ApplicationSet files for each cluster type.

    Args:
        shard: Shard configuration
        cluster_types: List of cluster types to render
        deploy_dir: Path to the deploy output directory
        base_dir: Base directory path (for loading base ApplicationSet)
    """
    environment = shard['environment']
    region_alias = shard['region_alias']
    config_revisions = shard.get('config_revision', {})

    # Create output directory
    output_dir = deploy_dir / environment / region_alias / 'argocd'
    output_dir.mkdir(parents=True, exist_ok=True)

    # Process each cluster type
    for cluster_type in cluster_types:
        config_revision = config_revisions.get(cluster_type)  # Get cluster-specific commit hash (may be None)

        applicationset_data = create_applicationset_template(cluster_type, environment, region_alias, config_revision, base_dir)

        # Create cluster-type manifests directory (simplified structure)
        manifests_dir = output_dir / f'{cluster_type}-manifests'
        manifests_dir.mkdir(parents=True, exist_ok=True)

        # ApplicationSet goes in the manifests directory
        output_file = manifests_dir / 'applicationset.yaml'
        revision_info = config_revision[:8] if config_revision else "metadata.annotations.git_revision"

        with open(output_file, 'w') as f:
            f.write(f"""# GENERATED FILE - DO NOT EDIT MANUALLY
#
# This file is automatically generated by the render script.
# To make changes:
# - Edit config.yaml for config_revision references
# - Then run: scripts/render.py
#
# Cluster Type: {cluster_type}
# Shard: {region_alias} ({environment})
# Config Revision: {revision_info}
# Generated: {Path(__file__).name}
#

""")
            yaml.dump(applicationset_data, f, default_flow_style=False, sort_keys=False, allow_unicode=True, width=float('inf'))

        print(f"  [OK] deploy/{environment}/{region_alias}/argocd/{cluster_type}-manifests/applicationset.yaml (Config Revision: {revision_info})")


def render_shard_values(
    shard: Dict[str, Any],
    cluster_types: List[str],
    base_dir: Path,
    deploy_dir: Path
) -> None:
    """Render values files for all cluster types for a shard.

    Args:
        shard: Shard configuration
        cluster_types: List of cluster types to render
        base_dir: Base directory containing cluster type directories
        deploy_dir: Path to the deploy output directory
    """
    environment = shard['environment']
    region_alias = shard['region_alias']

    print(f"Processing shard: {region_alias} ({environment})")

    # Create output directory
    output_dir = deploy_dir / environment / region_alias / 'argocd'
    output_dir.mkdir(parents=True, exist_ok=True)

    # Process each cluster type
    for cluster_type in cluster_types:
        # Get shard-specific values for this cluster type
        shard_cluster_values = shard.get('values', {}).get(cluster_type, {})

        # Get top-level values from the shard (not cluster-specific)
        shard_top_level_values = {k: v for k, v in shard.get('values', {}).items()
                                 if k not in cluster_types and k != 'global'}

        # Get global values that apply to this cluster type
        global_values = shard.get('values', {}).get('global', {})

        # Merge only the overrides: global_values <- shard_top_level <- shard_cluster_values
        # Do NOT include helm chart defaults - those stay in the charts
        override_values = deep_merge(global_values.copy(), shard_top_level_values)
        override_values = deep_merge(override_values, shard_cluster_values)

        # Always save values file (even if empty) as it's referenced in ApplicationSet
        output_file = output_dir / f'{cluster_type}-values.yaml'
        save_yaml(override_values, output_file, cluster_type, shard)
        if override_values:
            print(f"  [OK] deploy/{environment}/{region_alias}/argocd/{cluster_type}-values.yaml")
        else:
            print(f"  [OK] deploy/{environment}/{region_alias}/argocd/{cluster_type}-values.yaml (empty - no overrides)")


def render_shard_terraform(
    shard: Dict[str, Any],
    deploy_dir: Path
) -> None:
    """Generate terraform pipeline config files for a shard.

    Creates:
    - deploy/<env>/<region_alias>/terraform/regional.json
    - deploy/<env>/<region_alias>/terraform/management/<cluster_id>.json

    Args:
        shard: Shard configuration
        deploy_dir: Path to the deploy output directory
    """
    environment = shard['environment']
    region_alias = shard['region_alias']

    terraform_dir = deploy_dir / environment / region_alias / 'terraform'
    terraform_dir.mkdir(parents=True, exist_ok=True)

    # Generate regional.json from shard terraform_vars (already merged with sector)
    regional_file = terraform_dir / 'regional.json'
    regional_data = shard.get('terraform_vars', {}).copy()

    # Extract all management cluster account IDs for cross-account access configuration
    management_clusters = shard.get('management_clusters', [])
    mc_account_ids = [mc.get('account_id') for mc in management_clusters if mc.get('account_id')]

    # Add management cluster account IDs to regional config if any exist
    if mc_account_ids:
        regional_data['management_cluster_account_ids'] = mc_account_ids

    # Add metadata at the beginning
    regional_data_with_metadata = {
        "_generated": "DO NOT EDIT - Generated by scripts/render.py from config.yaml",
        **regional_data
    }

    with open(regional_file, 'w') as f:
        json.dump(regional_data_with_metadata, f, indent=2)
        f.write('\n')  # Add trailing newline

    print(f"  [OK] deploy/{environment}/{region_alias}/terraform/regional.json")

    # Generate management cluster configs
    management_clusters = shard.get('management_clusters', [])
    if management_clusters:
        management_dir = terraform_dir / 'management'
        management_dir.mkdir(parents=True, exist_ok=True)

        for mc in management_clusters:
            # Validate cluster_id is present and non-empty
            cluster_id = mc.get('cluster_id')
            if not cluster_id:
                raise ValueError(
                    f"Management cluster missing 'cluster_id' in shard {environment}/{region_alias}. "
                    f"Management cluster config: {mc}"
                )

            mc_file = management_dir / f'{cluster_id}.json'

            # Build MC terraform_vars by merging shard terraform_vars with MC-specific overrides
            shard_tf_vars = shard.get('terraform_vars', {}).copy()

            # MC-specific overrides (these override shard values)
            mc_overrides = {
                'account_id': mc.get('account_id'),
                'alias': cluster_id,
                'cluster_id': cluster_id,
                'regional_aws_account_id': shard.get('account_id'),
            }

            mc_data = deep_merge(shard_tf_vars, mc_overrides)

            # Add metadata at the beginning
            mc_data_with_metadata = {
                "_generated": "DO NOT EDIT - Generated by scripts/render.py from config.yaml",
                **mc_data
            }

            with open(mc_file, 'w') as f:
                json.dump(mc_data_with_metadata, f, indent=2)
                f.write('\n')  # Add trailing newline

            print(f"  [OK] deploy/{environment}/{region_alias}/terraform/management/{cluster_id}.json")


def cleanup_stale_files(shards: List[Dict[str, Any]], deploy_dir: Path) -> None:
    """Remove stale files from deploy directory that no longer exist in config.yaml.

    Args:
        shards: List of resolved shard configurations
        deploy_dir: Path to the deploy output directory
    """
    if not deploy_dir.exists():
        return

    # Build a set of valid shard paths (environment/region_alias)
    valid_shard_paths = {(shard['environment'], shard['region_alias']) for shard in shards}

    # Build a mapping of shard -> set of management cluster IDs
    shard_mc_map = {}
    for shard in shards:
        key = (shard['environment'], shard['region_alias'])
        # Only include non-empty cluster IDs
        mc_ids = {mc['cluster_id'] for mc in shard.get('management_clusters', []) if mc.get('cluster_id')}
        shard_mc_map[key] = mc_ids

    removed_count = 0

    # Scan deploy directory for environments
    for env_dir in deploy_dir.iterdir():
        if not env_dir.is_dir() or env_dir.name.startswith('.'):
            continue

        environment = env_dir.name

        # Scan for region directories within this environment
        for region_dir in env_dir.iterdir():
            if not region_dir.is_dir() or region_dir.name.startswith('.'):
                continue

            region_alias = region_dir.name
            shard_key = (environment, region_alias)

            # If this shard no longer exists in config.yaml, remove the entire directory
            if shard_key not in valid_shard_paths:
                print(f"  [CLEANUP] Removing stale shard: deploy/{environment}/{region_alias}/")
                shutil.rmtree(region_dir)
                removed_count += 1
                continue

            # Check for stale management cluster files
            mc_dir = region_dir / 'terraform' / 'management'
            if mc_dir.exists():
                valid_mc_ids = shard_mc_map.get(shard_key, set())

                for mc_file in mc_dir.glob('*.json'):
                    # Extract cluster_id from filename (e.g., mc01-us-east-1.json -> mc01-us-east-1)
                    cluster_id = mc_file.stem

                    if cluster_id not in valid_mc_ids:
                        print(f"  [CLEANUP] Removing stale MC: deploy/{environment}/{region_alias}/terraform/management/{mc_file.name}")
                        mc_file.unlink()
                        removed_count += 1

    if removed_count > 0:
        print()


def main() -> int:
    """Main entry point for the script.

    Returns:
        Exit code (0 for success, 1 for error)
    """
    # Determine script location and project root
    script_dir = Path(__file__).parent
    project_root = script_dir.parent

    config_file = project_root / 'config.yaml'
    base_dir = project_root / 'argocd' / 'config'
    deploy_dir = project_root / 'deploy'

    if not config_file.exists():
        print(f"Error: Config file not found: {config_file}", file=sys.stderr)
        return 1

    # Load config
    config = load_yaml(config_file)

    # Resolve shards from config (merge sector defaults + template processing)
    shards = resolve_shards(config)
    if not shards:
        print("Error: No shards found in config.yaml", file=sys.stderr)
        return 1

    # Validate environment + region_alias uniqueness
    try:
        validate_shard_uniqueness(shards)
    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        return 1

    # Validate config revision references
    try:
        validate_config_revisions(shards)
    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        return 1

    # Discover cluster types
    cluster_types = get_cluster_types(base_dir)
    if not cluster_types:
        print("Error: No cluster types found", file=sys.stderr)
        return 1

    print(f"Found {len(shards)} shard(s)")
    print(f"Found cluster types: {', '.join(cluster_types)}")
    print()

    # Clean up stale files before rendering
    cleanup_stale_files(shards, deploy_dir)

    # Process each shard
    for shard in shards:
        environment = shard.get('environment')
        region_alias = shard.get('region_alias')

        if not (environment and region_alias):
            print(f"Error: config.yaml entry does not require environment and region_alias: {shard}", file=sys.stderr)
            return 1

        render_shard_values(shard, cluster_types, base_dir, deploy_dir)
        render_shard_applicationsets(shard, cluster_types, deploy_dir, base_dir)
        render_shard_terraform(shard, deploy_dir)
        print()

    print("[OK] Rendering complete")
    return 0


if __name__ == '__main__':
    sys.exit(main())
