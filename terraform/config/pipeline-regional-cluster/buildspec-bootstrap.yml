version: 0.2

env:
  shell: bash

phases:
  install:
    commands:
      - echo "Installing dependencies..."
      - yum install -y unzip python3 jq gnupg2
      - pip3 install boto3 pyyaml
      - echo "Installing kubectl..."
      - |
        set -euo pipefail
        KUBECTL_VERSION="v1.31.0"

        # Download kubectl binary and checksum
        echo "Downloading kubectl ${KUBECTL_VERSION}..."
        curl -sSfLO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
        curl -sSfLO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl.sha256"

        # Verify checksum
        echo "Verifying kubectl checksum..."
        if ! echo "$(cat kubectl.sha256)  kubectl" | sha256sum -c -; then
          echo "❌ kubectl checksum verification failed"
          exit 1
        fi
        echo "✓ kubectl checksum verified"

        # Install kubectl
        chmod +x kubectl
        mv kubectl /usr/local/bin/
        rm -f kubectl.sha256

        kubectl version --client
        echo "✓ kubectl installation verified"
      - chmod +x scripts/dev/validate-argocd-config.sh scripts/bootstrap-argocd.sh
      - echo "Installing Terraform (needed to read outputs)..."
      - |
        set -euo pipefail
        TF_VERSION="1.14.3"
        TF_PACKAGE="terraform_${TF_VERSION}_linux_amd64.zip"
        TF_BASE_URL="https://releases.hashicorp.com/terraform/${TF_VERSION}"

        # Download Terraform package and verification files
        echo "Downloading Terraform ${TF_VERSION}..."
        curl -sSfO "${TF_BASE_URL}/${TF_PACKAGE}"
        curl -sSfO "${TF_BASE_URL}/terraform_${TF_VERSION}_SHA256SUMS"
        curl -sSfO "${TF_BASE_URL}/terraform_${TF_VERSION}_SHA256SUMS.sig"

        # Import HashiCorp GPG key
        echo "Importing HashiCorp GPG key..."
        gpg --batch --keyserver keyserver.ubuntu.com --recv-keys \
          C874011F0AB405110D02105534365D9472D7468F || \
        gpg --batch --keyserver keys.openpgp.org --recv-keys \
          C874011F0AB405110D02105534365D9472D7468F || \
        gpg --batch --keyserver pgp.mit.edu --recv-keys \
          C874011F0AB405110D02105534365D9472D7468F

        # Verify GPG signature on SHA256SUMS
        echo "Verifying GPG signature..."
        if ! gpg --batch --verify "terraform_${TF_VERSION}_SHA256SUMS.sig" "terraform_${TF_VERSION}_SHA256SUMS"; then
          echo "❌ GPG signature verification failed"
          exit 1
        fi
        echo "✓ GPG signature verified"

        # Verify checksum of Terraform package
        echo "Verifying SHA256 checksum..."
        if ! grep "${TF_PACKAGE}" "terraform_${TF_VERSION}_SHA256SUMS" | sha256sum -c -; then
          echo "❌ Checksum verification failed"
          exit 1
        fi
        echo "✓ Checksum verified"

        # Install Terraform
        echo "Installing Terraform..."
        unzip -o "${TF_PACKAGE}" -d /tmp/tf-bin
        mv /tmp/tf-bin/terraform /usr/local/bin/

        # Cleanup
        rm -f "${TF_PACKAGE}" "terraform_${TF_VERSION}_SHA256SUMS" "terraform_${TF_VERSION}_SHA256SUMS.sig"

        terraform version
        echo "✓ Terraform installation verified"

  build:
    commands:
      - echo "Bootstrapping ArgoCD on Regional Cluster..."
      - |
        set -euo pipefail

        # Function to assume role and export credentials with error handling
        assume_role_and_export_creds() {
            local ROLE_ARN=$1
            local SESSION_NAME=$2

            echo "Assuming role: $ROLE_ARN"

            # Capture both stdout and stderr
            local TEMP_FILE=$(mktemp)
            if ! TEMP_CREDS=$(aws sts assume-role \
                --role-arn "$ROLE_ARN" \
                --role-session-name "$SESSION_NAME" \
                --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]' \
                --output text 2>"$TEMP_FILE"); then
                echo "ERROR: Failed to assume role $ROLE_ARN" >&2
                echo "AWS CLI error output:" >&2
                cat "$TEMP_FILE" >&2
                rm -f "$TEMP_FILE"
                return 1
            fi
            rm -f "$TEMP_FILE"

            # Validate credentials are not empty
            if [ -z "$TEMP_CREDS" ]; then
                echo "ERROR: Received empty credentials from assume-role for $ROLE_ARN" >&2
                return 1
            fi

            # Export credentials
            export AWS_ACCESS_KEY_ID=$(echo $TEMP_CREDS | awk '{print $1}')
            export AWS_SECRET_ACCESS_KEY=$(echo $TEMP_CREDS | awk '{print $2}')
            export AWS_SESSION_TOKEN=$(echo $TEMP_CREDS | awk '{print $3}')

            # Validate all three parts are present
            if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ] || [ -z "$AWS_SESSION_TOKEN" ]; then
                echo "ERROR: Failed to parse credentials from assume-role output for $ROLE_ARN" >&2
                echo "Credentials format may be invalid" >&2
                return 1
            fi

            echo "Successfully assumed role: $ROLE_ARN"
            return 0
        }

        # Function to ensure state bucket exists with proper security settings
        ensure_state_bucket() {
            local BUCKET_NAME=$1
            local BUCKET_REGION=$2
            local TARGET_ACCOUNT=$3

            echo "Ensuring state bucket exists: $BUCKET_NAME in account $TARGET_ACCOUNT"

            # Assume role in target account to create bucket
            if ! assume_role_and_export_creds \
                "arn:aws:iam::${TARGET_ACCOUNT}:role/OrganizationAccountAccessRole" \
                "terraform-state-bucket-creation"; then
                echo "ERROR: Cannot proceed without valid credentials for account $TARGET_ACCOUNT" >&2
                return 1
            fi

            # Check if bucket exists
            if ! aws s3api head-bucket --bucket "$BUCKET_NAME" 2>/dev/null; then
                echo "Creating state bucket $BUCKET_NAME in $BUCKET_REGION..."

                if [ "$BUCKET_REGION" == "us-east-1" ]; then
                    aws s3api create-bucket --bucket "$BUCKET_NAME" --region "$BUCKET_REGION"
                else
                    aws s3api create-bucket --bucket "$BUCKET_NAME" --region "$BUCKET_REGION" \
                        --create-bucket-configuration LocationConstraint="$BUCKET_REGION"
                fi

                # Enable versioning
                aws s3api put-bucket-versioning --bucket "$BUCKET_NAME" \
                    --versioning-configuration Status=Enabled

                # Enable encryption
                aws s3api put-bucket-encryption --bucket "$BUCKET_NAME" \
                    --server-side-encryption-configuration \
                    '{"Rules": [{"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}]}'

                # Block public access
                aws s3api put-public-access-block --bucket "$BUCKET_NAME" \
                    --public-access-block-configuration \
                    "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"

                echo "✓ State bucket created and configured"
            else
                echo "✓ State bucket already exists"
            fi

            # Unset assumed role credentials
            unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN
        }

        bootstrap_target() {
            local ACCOUNT_ID=$1
            local REGION=$2
            local ALIAS=$3

            echo "---------------------------------------------------"
            echo "Bootstrapping ArgoCD: $ALIAS ($ACCOUNT_ID) in $REGION"

            local ROLE_ARN="arn:aws:iam::${ACCOUNT_ID}:role/OrganizationAccountAccessRole"

            # Set Terraform backend configuration (state stored in target account)
            export TF_STATE_BUCKET="terraform-state-${ACCOUNT_ID}"
            export TF_STATE_KEY="regional-cluster/${ALIAS}.tfstate"
            export TF_STATE_REGION=$REGION

            # Ensure state bucket exists in target account
            ensure_state_bucket "$TF_STATE_BUCKET" "$TF_STATE_REGION" "$ACCOUNT_ID"

            # Create override.tf to assume role for reading state AND accessing EKS
            echo "Creating override.tf for target account access..."
            echo 'provider "aws" {' > terraform/config/regional-cluster/override.tf
            echo "  region = \"${REGION}\"" >> terraform/config/regional-cluster/override.tf
            echo "  assume_role {" >> terraform/config/regional-cluster/override.tf
            echo "    role_arn = \"${ROLE_ARN}\"" >> terraform/config/regional-cluster/override.tf
            echo "  }" >> terraform/config/regional-cluster/override.tf
            echo '}' >> terraform/config/regional-cluster/override.tf

            echo "Initializing Terraform to enable output reading..."
            (
                cd terraform/config/regional-cluster
                terraform init -reconfigure \
                    -backend-config="bucket=${TF_STATE_BUCKET}" \
                    -backend-config="key=${TF_STATE_KEY}" \
                    -backend-config="region=${TF_STATE_REGION}" \
                    -backend-config="use_lockfile=true"
            )

            # Export ASSUME_ROLE_ARN for bootstrap script
            # The script will assume role AFTER reading terraform outputs
            # (outputs must be read with central account creds since state is in central account)
            export ASSUME_ROLE_ARN="${ROLE_ARN}"

            echo "Validating ArgoCD configuration..."
            ./scripts/dev/validate-argocd-config.sh regional-cluster

            echo "Checking terraform outputs are available..."
            (
                cd terraform/config/regional-cluster
                if ! terraform output -json > /tmp/tf-outputs.json 2>&1; then
                    echo "❌ Failed to read terraform outputs"
                    cat /tmp/tf-outputs.json
                    exit 1
                fi
                echo "✓ Terraform outputs available:"
                jq 'keys' /tmp/tf-outputs.json
            )

            echo "Bootstrapping ArgoCD..."
            ./scripts/bootstrap-argocd.sh regional-cluster "${TARGET_ENVIRONMENT}" "${TARGET_ALIAS}" "${REGION}" 2>&1 | tee /tmp/bootstrap.log
            BOOTSTRAP_EXIT_CODE=${PIPESTATUS[0]}

            echo ""
            echo "=== Bootstrap Script Log ==="
            cat /tmp/bootstrap.log
            echo "=== End Bootstrap Log ==="
            echo ""

            if [ $BOOTSTRAP_EXIT_CODE -ne 0 ]; then
                echo "❌ Bootstrap script failed with exit code $BOOTSTRAP_EXIT_CODE"
                exit 1
            fi

            echo "Verifying ArgoCD deployment..."
            # The bootstrap script handles kubeconfig setup with role assumption
            kubectl get pods -n argocd || echo "ArgoCD pods not yet ready"

            # Cleanup
            echo "Cleaning up..."
            rm -f terraform/config/regional-cluster/override.tf

            unset ASSUME_ROLE_ARN
        }

        # This buildspec is called by the pipeline with TARGET_* environment variables
        if [[ -n "$TARGET_ACCOUNT_ID" && -n "$TARGET_REGION" && -n "$TARGET_ALIAS" ]]; then
            echo "Using pipeline configuration (TARGET_* variables)"
            bootstrap_target "$TARGET_ACCOUNT_ID" "$TARGET_REGION" "$TARGET_ALIAS"
        else
            echo "ERROR: TARGET_* variables not set. This pipeline should be created by the provisioner."
            exit 1
        fi

  post_build:
    commands:
      - echo "ArgoCD bootstrap complete."
      - echo "Regional cluster is now fully provisioned and ready for use."

artifacts:
  files:
    - '**/*'
